{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train invoice object detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RwgRoyeYy0N4",
        "ZrhsBcHzyCDJ",
        "R6TRirJix3Fj",
        "SM1PXAm_yaRh",
        "D_CB7MFz22Ap",
        "kQbnR6PK_ASW",
        "EEN6ZOhTBsxP",
        "NOJ10_kU8QE0",
        "uG3wafQhJEoW",
        "DBigxvqeJI8O",
        "2wL4pEaZ613V",
        "NfhsbRl39F65",
        "BlXJkV-5LnUg"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deybvagm/invoice-object-detection/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8lu8CB39yW8",
        "colab_type": "text"
      },
      "source": [
        "# Training faster RCNN and SSD object detection models\n",
        "\n",
        "This notebook train this two models using the Tensorflow Object Detection API on an invoice dataset that be found [here](https://rrc.cvc.uab.es/?ch=13). This notebook covers the following:\n",
        "- Configuration of the environment by installing Tensorflow Object Detection API\n",
        "- Creation of TF records based on images of invoiceswith their respective bounding boxes\n",
        "- Train of the models faster RCNN and SSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrgBGi3rD20z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy==1.17.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp1xgwsnsZsW",
        "colab_type": "code",
        "outputId": "6f3f46ec-7a7b-4597-85f9-03ce85b539e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZAPUErmshlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import imutils\n",
        "import os\n",
        "import cv2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF4EpsWPsk2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('tf version ', tf.__version__)\n",
        "print('np version ', np.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwgRoyeYy0N4",
        "colab_type": "text"
      },
      "source": [
        "### Environment configuration\n",
        "\n",
        "We need:\n",
        "- Tensorflow object detection API: For this we are going to clone the repository\n",
        "- We need to install some needed packages in the system\n",
        "- We need compile object detection API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3I-Mtk9zEkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git\n",
        "!git clone https://github.com/deybvagm/invoice-object-detection.git\n",
        "!mv invoice-object-detection res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYN1nFHry76e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install Cython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX1ebIGywR3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd '/content/models/'\n",
        "!git checkout c9f03bf6a8ae58b9ead119796a6c3cd9bd04d450\n",
        "!mv /content/res/resources/*.py '/content/models/research/object_detection/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piXZGJvc5on0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile model definition\n",
        "%cd '/content/models/research/'\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnUAfnTG0WDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the environment\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_73xJFdk042A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build\n",
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6eSNUo91naJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to test if everything is ok\n",
        "!python /content/models/research/object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMTqq4fyxAGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a symbolic link to avoid writting complete paths\n",
        "%cd '/content/'\n",
        "!ln -s '/content/models/research/object_detection/' object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTYpd0d3vzYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Object detection imports\n",
        "from object_detection.utils.dataset_util import bytes_list_feature\n",
        "from object_detection.utils.dataset_util import float_list_feature\n",
        "from object_detection.utils.dataset_util import int64_list_feature\n",
        "from object_detection.utils.dataset_util import int64_feature\n",
        "from object_detection.utils.dataset_util import bytes_feature\n",
        "from object_detection.utils import label_map_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrhsBcHzyCDJ",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "In order that Tensorflow could understand the data we have we need to create standard Tensorflow Records that basically have the following info:\n",
        "- The image (encoded)\n",
        "- Bounding boxes coordinates\n",
        "- Format: JPG, PNG, etc\n",
        "- Labels\n",
        "- Image dimensions (width, height)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ0BdG2RyE4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFAnnotation:\n",
        "  def __init__(self):\n",
        "    # initialize the bounding box + label lists\n",
        "    self.xMins = []\n",
        "    self.xMaxs = []\n",
        "    self.yMins = []\n",
        "    self.yMaxs = []\n",
        "    self.textLabels = []\n",
        "    self.classes = []\n",
        "    self.difficult = []\n",
        "\n",
        "    # initialize additional variables, including the image\n",
        "    # itself, spatial dimensions, encoding, and filename\n",
        "    self.image = None\n",
        "    self.width = None\n",
        "    self.height = None\n",
        "    self.encoding = None\n",
        "    self.filename = None\n",
        "\n",
        "  def build(self):\n",
        "    # encode the attributes using their respective TensorFlow\n",
        "    # encoding function\n",
        "    w = int64_feature(self.width)\n",
        "    h = int64_feature(self.height)\n",
        "    filename = bytes_feature(self.filename.encode(\"utf8\"))\n",
        "    encoding = bytes_feature(self.encoding.encode(\"utf8\"))\n",
        "    image = bytes_feature(self.image)\n",
        "    xMins = float_list_feature(self.xMins)\n",
        "    xMaxs = float_list_feature(self.xMaxs)\n",
        "    yMins = float_list_feature(self.yMins)\n",
        "    yMaxs = float_list_feature(self.yMaxs)\n",
        "    textLabels = bytes_list_feature(self.textLabels)\n",
        "    classes = int64_list_feature(self.classes)\n",
        "    difficult = int64_list_feature(self.difficult)\n",
        "\n",
        "    # construct the TensorFlow-compatible data dictionary\n",
        "    data = {\n",
        "        \"image/height\": h,\n",
        "        \"image/width\": w,\n",
        "        \"image/filename\": filename,\n",
        "        \"image/source_id\": filename,\n",
        "        \"image/encoded\": image,\n",
        "        \"image/format\": encoding,\n",
        "        \"image/object/bbox/xmin\": xMins,\n",
        "        \"image/object/bbox/xmax\": xMaxs,\n",
        "        \"image/object/bbox/ymin\": yMins,\n",
        "        \"image/object/bbox/ymax\": yMaxs,\n",
        "        \"image/object/class/text\": textLabels,\n",
        "        \"image/object/class/label\": classes,\n",
        "        \"image/object/difficult\": difficult,\n",
        "    }\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6TRirJix3Fj",
        "colab_type": "text"
      },
      "source": [
        "### Creating TF Records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fETsXz56yPmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#donwload training data from S3 (without records)\n",
        "!wget https://invoiceproject.s3-us-west-2.amazonaws.com/invoice_no_records.tar.gz\n",
        "!tar -zxvf invoice_no_records.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17mBBSk-vX3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conf():\n",
        "  def __init__(self):\n",
        "    self.BASE_PATH = \"invoice\"\n",
        "\n",
        "    self.ANNOT_PATH = os.path.sep.join([self.BASE_PATH, \"annotations.csv\"])\n",
        "\n",
        "    self.TRAIN_RECORD = os.path.sep.join([self.BASE_PATH,\"records/training.record\"])\n",
        "\n",
        "    self.TEST_RECORD = os.path.sep.join([self.BASE_PATH,\"records/testing.record\"])\n",
        "\n",
        "    self.CLASSES_FILE = os.path.sep.join([self.BASE_PATH,\"records/classes.pbtxt\"])\n",
        "\n",
        "    self.TEST_SIZE = 0.25\n",
        "\n",
        "    self.CLASSES = {\"header\": 1, \"date\": 2, \"total\": 3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awjLvRuc9EZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Conf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqmGk8Vc-fbZ",
        "colab_type": "code",
        "outputId": "ed795cc5-d43f-4bea-e930-4b5680461d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpviqzxX9TgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_records(config):  \n",
        "  f = open(config.CLASSES_FILE, \"w\")\n",
        "  for (k, v) in config.CLASSES.items():\n",
        "    item = (\"item {\\n\"\n",
        "      \"\\tid: \" + str(v) + \"\\n\"\n",
        "      \"\\tname: '\" + k + \"'\\n\"\n",
        "      \"}\\n\")\n",
        "    f.write(item)\n",
        "  f.close()\n",
        "  D = {}\n",
        "  rows = open(config.ANNOT_PATH).read().strip().split(\"\\n\")\n",
        "  discarded_rows = 0\n",
        "  discarded_labels = 0\n",
        "  print('rows in file: ', len(rows))\n",
        "  \n",
        "  for row in rows[1:]:\n",
        "    row = row.split(\",\")\n",
        "    if len(row) < 11:\n",
        "      discarded_rows += 1\n",
        "      continue  \n",
        "\n",
        "    imagePath = row[0]\n",
        "    startX = float(row[6].split(\":\")[1].strip(\"\\\"'}\"))\n",
        "    startY = float(row[7].split(\":\")[1].strip(\"\\\"'}\"))\n",
        "    width = float(row[8].split(\":\")[1].strip(\"\\\"'}\"))\n",
        "    height = float(row[9].split(\":\")[1].strip(\"\\\"'}\"))\n",
        "\n",
        "    endX = startX + width\n",
        "    endY = startY + height\n",
        "    label = row[-1].split(\":\")[1].strip(\"\\\"'}\")\n",
        "    \n",
        "    if label not in config.CLASSES:\n",
        "      discarded_labels += 1\n",
        "      continue\n",
        "    \n",
        "    p = os.path.sep.join([config.BASE_PATH, imagePath])\n",
        "    b = D.get(p, [])\n",
        "\n",
        "    b.append((label, (startX, startY, endX, endY)))\n",
        "    D[p] = b\n",
        "\n",
        "  print('discarded rows: ', discarded_rows)\n",
        "  print('discarded labels: ', discarded_labels)\n",
        "  \n",
        "  (trainKeys, testKeys) = train_test_split(list(D.keys()),\n",
        "  test_size=config.TEST_SIZE, random_state=42)\n",
        "  \n",
        "  datasets = [\n",
        "    (\"train\", trainKeys, config.TRAIN_RECORD),\n",
        "    (\"test\", testKeys, config.TEST_RECORD)\n",
        "  ]\n",
        "  \n",
        "  for (dType, keys, outputPath) in datasets:    \n",
        "    print(\"[INFO] processing '{}'...\".format(dType))\n",
        "    writer = tf.python_io.TFRecordWriter(outputPath)\n",
        "    total = 0\n",
        "    \n",
        "    for k in keys:\n",
        "      encoded = tf.gfile.GFile(k, \"rb\").read()\n",
        "      encoded = bytes(encoded)      \n",
        "      pilImage = Image.open(k)\n",
        "      (w, h) = pilImage.size[:2]\n",
        "      \n",
        "      filename = k.split(os.path.sep)[-1]\n",
        "      encoding = filename[filename.rfind(\".\") + 1:]\n",
        "      \n",
        "      tfAnnot = TFAnnotation()\n",
        "      tfAnnot.image = encoded\n",
        "      tfAnnot.encoding = encoding\n",
        "      tfAnnot.filename = filename\n",
        "      tfAnnot.width = w\n",
        "      tfAnnot.height = h\n",
        "      \n",
        "      for (label, (startX, startY, endX, endY)) in D[k]:\n",
        "        # TensorFlow assumes all bounding boxes are in the\n",
        "        # range [0, 1] so we need to scale them\n",
        "        xMin = startX / w\n",
        "        xMax = endX / w\n",
        "        yMin = startY / h\n",
        "        yMax = endY / h\n",
        "\n",
        "        #####JUST INSPECTION- REMOVE\n",
        "\n",
        "        # load the input image from disk and denormalize the\n",
        "        # bounding box coordinates\n",
        "        #image = cv2.imread(k)\n",
        "        #startX = int(xMin * w)\n",
        "        #startY = int(yMin * h)\n",
        "        #endX = int(xMax * w)\n",
        "        #endY = int(yMax * h)\n",
        "\n",
        "        # draw the bounding box on the image\n",
        "        #cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
        "        #cv2.rectangle(image, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
        "        #cv2.putText(image, label, (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
        "\n",
        "        # show the output image\n",
        "        #cv2.imshow(\"Image\", image)\n",
        "        #cv2.waitKey(0)\n",
        "\n",
        "        #####JUST INSPECTION- REMOVE\n",
        "\n",
        "        tfAnnot.xMins.append(xMin)\n",
        "        tfAnnot.xMaxs.append(xMax)\n",
        "        tfAnnot.yMins.append(yMin)\n",
        "        tfAnnot.yMaxs.append(yMax)\n",
        "        tfAnnot.textLabels.append(label.encode(\"utf8\"))\n",
        "        tfAnnot.classes.append(config.CLASSES[label])\n",
        "        tfAnnot.difficult.append(0)\n",
        "\n",
        "        # increment the total number of examples\n",
        "        total += 1\n",
        "      \n",
        "      features = tf.train.Features(feature=tfAnnot.build())\n",
        "      example = tf.train.Example(features=features)\n",
        "      \n",
        "      writer.write(example.SerializeToString())    \n",
        "    writer.close()\n",
        "    print(\"[INFO] {} examples saved for '{}'\".format(total, dType))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Aydr30W-ny9",
        "colab_type": "code",
        "outputId": "9744e41e-3974-4c44-89a5-645588948abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "create_records(config)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rows in file:  1873\n",
            "discarded rows:  0\n",
            "discarded labels:  4\n",
            "[INFO] processing 'train'...\n",
            "[INFO] 1400 examples saved for 'train'\n",
            "[INFO] processing 'test'...\n",
            "[INFO] 468 examples saved for 'test'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn7u9FRSxUtT",
        "colab_type": "text"
      },
      "source": [
        "## Faster RCNN model\n",
        "\n",
        "In this section we are going to train the faster RCNN mode with a pretrained model. The steps are:\n",
        "\n",
        "- **Download the pretrained** model from Tensorflow model zoo.\n",
        "- **Create the configuration file** needed to perform finetunning.\n",
        "- **Launch Tensorboard** monitor training.\n",
        "- **Start the training** process.\n",
        "- **Export the model** for later predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1PXAm_yaRh",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the pretrained model from Tensorflow zoo\n",
        "Here we used a model that was trained on the COCO dataset taking as the base network **ResNet** architecture. Specifically we downloaded the `faster_rcnn_resnet101_coco` which you can see [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSds1EPjxupp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz\n",
        "!tar -zxvf faster_rcnn_resnet101_coco_2018_01_28.tar.gz\n",
        "!rm faster_rcnn_resnet101_coco_2018_01_28.tar.gz\n",
        "!mv faster_rcnn_resnet101_coco_2018_01_28 '/content/invoice/experiments/rcnn/training/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_CB7MFz22Ap",
        "colab_type": "text"
      },
      "source": [
        "### Creating the configuration file\n",
        "This is the file where you can configure things like training steps, the paths to the records, and other configuration for the finetunning process. The file we used, was originally taken from [this tensorflow repo](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs) with the name `faster_rcnn_resnet101_pets.config`. We choose the pets and not COCO because is easier, with less configuration to define. Basically the configuration we need to care about is:\n",
        "\n",
        "- `num_classes`: This value should be 3 as we have this number of classes (header, total and date)\n",
        "- `num_steps`: This value should be large if you want to perform a good training (between 20.000 and 200.000). For this particular demostration we are going to put 2.500\n",
        "- `fine_tune_checkpoint`: This is the path to the pretrained model and the value hsould be `/content/invoice/experiments/rcnn/training/faster_rcnn_resnet101_coco_2018_01_28/model.ckpt`\n",
        "- `input_path` for train input reader: this is the path to the record created previously for training. The value should be `/content/invoice/records/training.record`\n",
        "- `label_map_path`: The file where the class information is saved. This file was also generated during the creation of TF Records. The value should be `/content/invoice/records/classes.pbtxt`\n",
        "- `input_path` for test input reader: this is the path to the record created previously for training. The value should be `/content/invoice/records/testing.record`\n",
        "- `label_map_path` for testing: This is the same file reported for training. The value should be `/content/invoice/records/classes.pbtxt`\n",
        "- `num_examples`: This is the number of bounding boxes that we have for testing. This number is printed during the creation of TF Records and the values is `468`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdoXIQrO5eRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_conf = '/content/invoice/experiments/rcnn/training/faster_rcnn_invoice.config'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SL2pBXY5Gnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm '/content/invoice/experiments/rcnn/training/faster_rcnn_invoice.config'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q8gTcX445YV",
        "colab_type": "code",
        "outputId": "d4680ed4-e07a-4433-ae8e-c5cb6ff8f673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile {path_to_conf}\n",
        "model {\n",
        "  faster_rcnn {\n",
        "    num_classes: 3\n",
        "    image_resizer {\n",
        "      keep_aspect_ratio_resizer {\n",
        "        min_dimension: 600\n",
        "        max_dimension: 1024\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'faster_rcnn_resnet101'\n",
        "      first_stage_features_stride: 16\n",
        "    }\n",
        "    first_stage_anchor_generator {\n",
        "      grid_anchor_generator {\n",
        "        scales: [0.25, 0.5, 1.0, 2.0]\n",
        "        aspect_ratios: [0.5, 1.0, 2.0]\n",
        "        height_stride: 16\n",
        "        width_stride: 16\n",
        "      }\n",
        "    }\n",
        "    first_stage_box_predictor_conv_hyperparams {\n",
        "      op: CONV\n",
        "      regularizer {\n",
        "        l2_regularizer {\n",
        "          weight: 0.0\n",
        "        }\n",
        "      }\n",
        "      initializer {\n",
        "        truncated_normal_initializer {\n",
        "          stddev: 0.01\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    first_stage_nms_score_threshold: 0.0\n",
        "    first_stage_nms_iou_threshold: 0.7\n",
        "    first_stage_max_proposals: 300\n",
        "    first_stage_localization_loss_weight: 2.0\n",
        "    first_stage_objectness_loss_weight: 1.0\n",
        "    initial_crop_size: 14\n",
        "    maxpool_kernel_size: 2\n",
        "    maxpool_stride: 2\n",
        "    second_stage_box_predictor {\n",
        "      mask_rcnn_box_predictor {\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 1.0\n",
        "        fc_hyperparams {\n",
        "          op: FC\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.0\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            variance_scaling_initializer {\n",
        "              factor: 1.0\n",
        "              uniform: true\n",
        "              mode: FAN_AVG\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    second_stage_post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 0.0\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 300\n",
        "      }\n",
        "      score_converter: SOFTMAX\n",
        "    }\n",
        "    second_stage_localization_loss_weight: 2.0\n",
        "    second_stage_classification_loss_weight: 1.0\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 1\n",
        "  optimizer {\n",
        "    momentum_optimizer: {\n",
        "      learning_rate: {\n",
        "        manual_step_learning_rate {\n",
        "          initial_learning_rate: 0.0003\n",
        "          schedule {\n",
        "            step: 900000\n",
        "            learning_rate: .00003\n",
        "          }\n",
        "          schedule {\n",
        "            step: 1200000\n",
        "            learning_rate: .000003\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "    }\n",
        "    use_moving_average: false\n",
        "  }\n",
        "  gradient_clipping_by_norm: 10.0\n",
        "  fine_tune_checkpoint: \"/content/invoice/experiments/rcnn/training/faster_rcnn_resnet101_coco_2018_01_28/model.ckpt\"\n",
        "  from_detection_checkpoint: true\n",
        "  load_all_detection_checkpoint_vars: true\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 2500\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/invoice/records/training.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/invoice/records/classes.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  num_examples: 468\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/invoice/records/testing.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/invoice/records/classes.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/invoice/experiments/rcnn/training/faster_rcnn_invoice.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQbnR6PK_ASW",
        "colab_type": "text"
      },
      "source": [
        "### Launching Tensorboard\n",
        "\n",
        "Tensorboard won't show anything so far because the training process has not started yet, but once the training is running (next step) the Tensorboard should update and start to show the progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIjKuYXH_Gwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir '/content/invoice/experiments/rcnn/training/'\n",
        "\n",
        "except:\n",
        "    print (\"not in colab\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEN6ZOhTBsxP",
        "colab_type": "text"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_sri-XGBv-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd '/content'\n",
        "!python object_detection/model_main.py --alsologtostderr \\\n",
        "--pipeline_config_path invoice/experiments/rcnn/training/faster_rcnn_invoice.config \\\n",
        "--model_dir invoice/experiments/rcnn/training \\\n",
        "--num_train_steps=2500 \\\n",
        "--sample_1_of_n_eval_examples=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOJ10_kU8QE0",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the model\n",
        "In this step we are going to export the generated model for later predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxCUBE508ZkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python object_detection/export_inference_graph.py --input_type image_tensor \\\n",
        "--pipeline_config_path invoice/experiments/rcnn/training/faster_rcnn_invoice.config \\\n",
        "--trained_checkpoint_prefix invoice/experiments/rcnn/training/model.ckpt-2500 \\\n",
        "--output_directory invoice/experiments/rcnn/exported_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjgGfY1bA26C",
        "colab_type": "text"
      },
      "source": [
        "## SSD model\n",
        "\n",
        "Now we can just train another model, in this case a SSD based on a pretrained model using COCO dataset. The steps are the same as before:\n",
        "\n",
        "- **Download the pretrained** model from Tensorflow model zoo.\n",
        "- **Create the configuration file** needed to perform finetunning.\n",
        "- **Launch Tensorboard** to monitor training.\n",
        "- **Start the training** process.\n",
        "- **Export the model** for later predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG3wafQhJEoW",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the pretarined model\n",
        "\n",
        "Here we used a model that was trained on the COCO dataset taking the **Inception** as the base network to perform detection. Specifically we downloaded the `ssd_inception_v2_coco` which you can see [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAbeNQX2A5Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz\n",
        "!tar -zxvf ssd_inception_v2_coco_2018_01_28.tar.gz\n",
        "!rm ssd_inception_v2_coco_2018_01_28.tar.gz\n",
        "!mv ssd_inception_v2_coco_2018_01_28 '/content/invoice/experiments/ssd/training/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBigxvqeJI8O",
        "colab_type": "text"
      },
      "source": [
        "### Creating the configuration file\n",
        "\n",
        "This is the file where you can configure things like training steps, the paths to the records, and other configuration for the finetunning process. The file we used, was originally taken from [this tensorflow repo](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs) with the name `ssd_inception_v2_pets.config`. We choose the pets and not COCO because is easier, with less configuration to define. Basically the configuration we need to care about is:\n",
        "\n",
        "- `num_classes`: This value should be 3 as we have this number of classes (header, total and date)\n",
        "- `num_steps`: This value should be large if you want to perform a good training (between 20.000 and 200.000). For this particular demostration we are going to put 2.500\n",
        "- `fine_tune_checkpoint`: This is the path to the pretrained model and the value should be `/content/invoice/experiments/ssd/training/ssd_inception_v2_coco_2018_01_28/model.ckpt`\n",
        "- `input_path` for train input reader: this is the path to the record created previously for training. The value should be `/content/invoice/records/training.record`\n",
        "- `label_map_path`: The file where the class information is saved. This file was also generated during the creation of TF Records. The value should be `/content/invoice/records/classes.pbtxt`\n",
        "- `input_path` for test input reader: this is the path to the record created previously for training. The value should be `/content/invoice/records/testing.record`\n",
        "- `label_map_path` for testing: This is the same file reported for training. The value should be `/content/invoice/records/classes.pbtxt`\n",
        "- `num_examples`: This is the number of bounding boxes that we have for testing. This number is printed during the creation of TF Records and the values is `468`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgnAqhRoGK20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_conf = '/content/invoice/experiments/ssd/training/ssd_invoices.config'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtUbpxKiGMFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm '/content/invoice/experiments/ssd/training/ssd_invoices.config'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR7HXF-cBCwd",
        "colab_type": "code",
        "outputId": "750b1172-23b4-4240-f9d5-7dfc8bb926b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile {path_to_conf}\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 3\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "        reduce_boxes_in_lowest_layer: true\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 3\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_inception_v3'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.1\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.01,\n",
        "        }\n",
        "      }\n",
        "      override_base_feature_extractor_hyperparams: true\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 0\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 24\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/invoice/experiments/ssd/training/ssd_inception_v2_coco_2018_01_28/model.ckpt\"\n",
        "  from_detection_checkpoint: true\n",
        "  load_all_detection_checkpoint_vars: true\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 2500\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/invoice/records/training.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/invoice/records/classes.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  num_examples: 468\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/invoice/records/testing.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/invoice/records/classes.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/invoice/experiments/ssd/training/ssd_invoices.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wL4pEaZ613V",
        "colab_type": "text"
      },
      "source": [
        "### Launching Tensorboard\n",
        "\n",
        "Tensorboard won't show anything so far because the training process has not started yet, but once the training is running (next step) the Tensorboard should update and start to show the progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUH8_o18--m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir '/content/invoice/experiments/ssd/training/'\n",
        "\n",
        "except:\n",
        "    print (\"not in colab\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfhsbRl39F65",
        "colab_type": "text"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R90pPocxKcNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd '/content'\n",
        "!python object_detection/model_main.py --alsologtostderr \\\n",
        "--pipeline_config_path invoice/experiments/ssd/training/ssd_invoices.config \\\n",
        "--model_dir invoice/experiments/ssd/training \\\n",
        "--num_train_steps=2500 \\\n",
        "--sample_1_of_n_eval_examples=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlXJkV-5LnUg",
        "colab_type": "text"
      },
      "source": [
        "### To export the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ49V7wuLqLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python object_detection/export_inference_graph.py --input_type image_tensor \\\n",
        "--pipeline_config_path invoice/experiments/ssd/training/ssd_invoices.config \\\n",
        "--trained_checkpoint_prefix invoice/experiments/ssd/training/model.ckpt-2500 \\\n",
        "--output_directory invoice/experiments/ssd/exported_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgBxA85i-NlD",
        "colab_type": "text"
      },
      "source": [
        "## What's Next?\n",
        "\n",
        "In this notebook we trained two models (faster RCNN and SSD) for object detection on invoices, specifically to detect and localize the **header, the total and the date**. This was the process we followed:\n",
        "- We configured the environment by installing the Tensorflow Object Detection API\n",
        "- We downloaded the dataset and created the TF Records that are the standard way the API handles the images and bounding boxes\n",
        "- Then for each of the two models we:\n",
        "  * Downloaded a pretrained model (on COCO dataset) from Tensorflow Model Zoo\n",
        "  * Created a conf define parameters for finetunning\n",
        "  * Launch Tensorboard to monitor the training process\n",
        "  * Executed the training process\n",
        "  * Exported the trained model for later predictions\n",
        "\n",
        "Now if we are interested in making predictions we can follow the notebook provided in this repository to that end"
      ]
    }
  ]
}